{"cells":[{"cell_type":"markdown","source":["Randorm Forest class defintions for:\n * `RFRBaseModel`\n * `RFFExperimentModel`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"414e0634-9fed-49cb-88a8-58ae189562dd"}}},{"cell_type":"code","source":["import os\nimport numpy as np\nimport mlflow.sklearn\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom mlflow.tracking import MlflowClient"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a11b0dd0-86ca-46f2-9ff4-68fea9ec02e4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class RFRBaseModel():\n\n    def __init__(self, params={}):\n        \"\"\"\n        Construtor for the RandomForestRegressor\n        :param params: dictionary to RandomForestRegressor\n        \"\"\"\n        self._params = params\n        self._rf = RandomForestRegressor(**params)\n\n    @classmethod\n    def new_instance(cls, params={}):\n        return cls(params)\n\n    @property \n    def model(self):\n        \"\"\"\n        Getter for the model\n        :return: return the model\n        \"\"\"\n        return self._rf\n\n    @property\n    def params(self):\n      \"\"\"\n      Getter for model parameters\n      returns: Dictionary of model parameters\n      \"\"\"\n      return self._params\n    \n    def mlflow_run(self, df, r_name=\"Lab-3: Baseline RF Model\"):\n        \"\"\"\n        This method trains, computes metrics, and logs all metrics, parameters,\n        and artifacts for the current run\n        :param df: pandas dataFrame\n        :param r_name: Name of the experiment as logged by MLflow\n        :return: Tuple of MLflow experimentID, runID\n        \"\"\"\n        with mlflow.start_run(run_name=r_name) as run:\n            X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n            self._rf.fit(X_train, y_train)\n            predictions = self._rf.predict(X_test)\n\n            # Log model and parameters\n            mlflow.sklearn.log_model(self.model, \"random-forest-model\")\n            mlflow.log_params(self.params)\n\n            # Create metrics\n            mae = metrics.mean_absolute_error(y_test, predictions)\n            mse = metrics.mean_squared_error(y_test, predictions)\n            rmse = np.sqrt(mse)\n            r2 = metrics.r2_score(y_test, predictions)\n\n            # Log metrics\n            mlflow.log_metric(\"mae\", mae)\n            mlflow.log_metric(\"mse\", mse)\n            mlflow.log_metric(\"rmse\", rmse)\n            mlflow.log_metric(\"r2\", r2)\n\n            runID = run.info.run_uuid\n            experimentID = run.info.experiment_id\n\n            # print some data\n            print(\"-\" * 100)\n            print(\"Inside MLflow {} Run with run_id {} and experiment_id {}\".format(r_name, runID, experimentID))\n            print(\"Estimator trees        :\", self.params[\"n_estimators\"])\n            print('Mean Absolute Error    :', mae)\n            print('Mean Squared Error     :', mse)\n            print('Root Mean Squared Error:', rmse)\n            print('R2                     :', r2)\n\n            return (experimentID, runID)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d13e8fc0-0d12-4821-b361-718397b4c265"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["displayHTML(\"\"\"\n<div>Declared RandomForestModel Base Class with methods:</div>\n<li>Declared <b style=\"color:green\">model()</b> returns existing instance of Random Forest Model</li>\n<li>Declared <b style=\"color:green\">new_instance(params={...})</b> returns a new instance of RandomForestClassifierModel</li> \n<li>Declared <b style=\"color:green\"> mlflow_run(DataFrame, run_name=\"name\")</b> returns experiment_ID, run_ID</li>\n\n<br/>\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6419532-5519-40b4-a5aa-829fe674dbd5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class RFFExperimentModel(RFRBaseModel):\n    \"\"\"\n    Constructor for the Experimental RandomForestRegressor.\n    \"\"\"\n    def __int__(self, params):\n        \"\"\"\n        Call the superclass initializer\n        :param params: parameters for the RandomForestRegressor instance\n        :return: None\n        \"\"\"\n        super(RFRBaseModel, self).__init__(params)\n\n    def mlflow_run(self, df, r_name=\"Lab-4:RF Experiment Model\"):\n        \"\"\"\n        Override the base class mlflow_run for this epxerimental runs\n        This method trains the model, evaluates, computes the metrics, logs\n        all the relevant metrics, artifacts, and models.\n        :param df: pandas dataFrame\n        :param r_name: name of the experiment run\n        :return:  MLflow Tuple (ExperimentID, runID)\n        \"\"\"\n\n        with mlflow.start_run(run_name=r_name) as run:\n            # get experimentalID and runID\n            runID = run.info.run_uuid\n            experimentID = run.info.experiment_id\n            \n            # split train/test and train the model\n            X_train, X_test, y_train, y_test = train_test_split(df.drop([\"price\"], axis=1), df[[\"price\"]].values.ravel(), random_state=42)\n            self._rf.fit(X_train, y_train)\n            predictions = self._rf.predict(X_test)\n            # create an Actual vs Predicted DataFrame\n            df_preds = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': predictions.flatten()})\n\n            # Log model and parameters\n            mlflow.sklearn.log_model(self.model, \"random-forest-model\")\n\n            # Note we are logging as a dictionary of all params instead of logging each parameter\n            mlflow.log_params(self.params)\n\n\n            # Create metrics\n            mse = metrics.mean_squared_error(y_test, predictions)\n            rmse = np.sqrt(mse)\n            mae = metrics.mean_absolute_error(y_test, predictions)\n            r2 = metrics.r2_score(y_test, predictions)\n\n            # Log metrics\n            mlflow.log_metric(\"mse\", mse)\n            mlflow.log_metric(\"mae\", mae)\n            mlflow.log_metric(\"rmse\", rmse)\n            mlflow.log_metric(\"r2\", r2)\n            \n            # Log prediciton/actual values in file as a feature artifact\n            temp_file_name = Utils.get_temporary_directory_path(\"predicted-actual-\", \".csv\")\n            temp_name = temp_file_name.name\n            try:\n                df_preds.to_csv(temp_name, index=False)\n                mlflow.log_artifact(temp_name, \"predicted-actual-files\")\n            finally:\n                temp_file_name.close()  # Delete the temp file\n\n            # Create feature importance and save them as artifact\n            # This allows us to remove least important features from the dataset\n            # with each iteration if they don't have any effect on the predictive power of\n            # the prediction.\n            importance = pd.DataFrame(list(zip(df.columns, self._rf.feature_importances_)),\n                                      columns=[\"Feature\", \"Importance\"]\n                                      ).sort_values(\"Importance\", ascending=False)\n\n            # Log importance file as feature artifact\n            temp_file_name = Utils.get_temporary_directory_path(\"feature-importance-\", \".csv\")\n            temp_name = temp_file_name.name\n            try:\n                importance.to_csv(temp_name, index=False)\n                mlflow.log_artifact(temp_name, \"feature-importance-files\")\n            finally:\n                temp_file_name.close()  # Delete the temp file\n\n            # Create residual plots and image directory\n            # Residuals R = observed value - predicted value\n            (plt, fig, ax) = Utils.plot_residual_graphs(predictions, y_test, \"Predicted values for Price ($)\", \"Residual\",\n                                                  \"Residual Plot\")\n\n            # Log residuals images\n            temp_file_name = Utils.get_temporary_directory_path(\"residuals-\", \".png\")\n            temp_name = temp_file_name.name\n            try:\n                fig.savefig(temp_name)\n                mlflow.log_artifact(temp_name, \"residuals-plots\")\n            finally:\n                temp_file_name.close()  # Delete the temp file\n\n            print(\"-\" * 100)\n            print(\"Inside MLflow {} Run with run_id {} and experiment_id {}\".format(r_name, runID, experimentID))\n            print(\"  mse: {}\".format(mse))\n            print(\" rmse: {}\".format(rmse))\n            print(\"  mae: {}\".format(mae))\n            print(\"  R2 : {}\".format(r2))\n\n            return (experimentID, runID)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c0b08cb-3c93-4121-a88e-30c6f96df13a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["displayHTML(\"\"\"\n<div>Declared RFFExperimentModel Extended Class with methods:</div>\n<li>Declared <b style=\"color:green\">model()</b> returns existing instance of Random Forest Model</li>\n<li>Declared <b style=\"color:green\">new_instance(params={...})</b> returns a new instance of RandomForestClassifierModel</li> \n<li>Declared <b style=\"color:green\"> mlflow_run(DataFrame, run_name=\"name\")</b> returns experiment_ID, run_ID</li>\n\n<br/>\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5007131b-b142-4d1f-9217-55b44a76fd68"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"rfr_regression_base_exp_cls","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2387317862617817}},"nbformat":4,"nbformat_minor":0}
